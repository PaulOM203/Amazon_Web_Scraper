{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b8274a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the necessary libraries\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e78781a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining each function we will use\n",
    "\n",
    "def get_title(soup):\n",
    "    try:\n",
    "        title = soup.find(\"span\", {\"id\":'productTitle'}).text.strip()\n",
    "    except AttributeError:\n",
    "        title = ''\n",
    "    return title\n",
    "\n",
    "def get_price(soup):\n",
    "    try:\n",
    "        price = float(soup.find(\"span\", {\"class\":'a-price-whole'}).text.strip().replace(',', '')) + (float(soup.find(\"span\", {\"class\":'a-price-fraction'}).text.strip())/100)\n",
    "    except AttributeError:\n",
    "        price = ''\n",
    "    return price\n",
    "\n",
    "def get_rating(soup):\n",
    "    try:\n",
    "        rating = soup.find(\"span\", {\"class\":'a-icon-alt'}).text.strip()\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            rating = soup.find(\"span\", {\"class\":'a-size-base a-color-base'}).text.strip()\n",
    "        except:\n",
    "            rating = ''\n",
    "    return rating\n",
    "\n",
    "def get_reviews(soup):\n",
    "    try:\n",
    "        reviews = soup.find(\"span\", {\"id\":'acrCustomerReviewText'}).text.strip()\n",
    "    except AttributeError:\n",
    "        reviews = ''\n",
    "    return reviews\n",
    "\n",
    "def get_availability(soup):\n",
    "    try:\n",
    "        available = soup.find(\"div\", attrs={'id':'availability'})\n",
    "        available = available.find(\"span\").text.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        available = \"Not Available\"\t\n",
    "\n",
    "    return available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e265b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def amazon_scrapper(product_name):\n",
    "\n",
    "    #We use the product name provided by the user to complete and get the right URL from Amazon\n",
    "    searchterm = product_name.replace(' ', '+')\n",
    "    URL = f'https://www.amazon.com/s?k={searchterm}&ref=nb_sb_noss_1'\n",
    "    \n",
    "    #We need to use the headers data from our computer to access the webpage. You can find this on internet\n",
    "    headers = {\"User-Agent\": \"\", \n",
    "                     \"Accept-Encoding\": \"gzip, deflate, br, zstd\", \n",
    "                     \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\", \n",
    "                     \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "\n",
    "    #We use requests and BeautifulSoup to access and parse the webpage\n",
    "    page = requests.get(URL, headers=headers)\n",
    "    soup1 = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    #Create a kind of list of each product link with BeautifulSoup\n",
    "    links = soup1.find_all('a', {'class': 'a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal'})\n",
    "\n",
    "    #Now we get the actual links of the previous 'list' and put them in an actual list of links\n",
    "    link_list = []\n",
    "    for link in links:\n",
    "        link_list.append('https://www.amazon.com' + link.get('href'))\n",
    "\n",
    "    #Create an empty dictionary to save the data we are going to extract\n",
    "    product_dic = {'Title': [], 'Price': [], 'Rating': [], 'Reviews': [], 'Availability': [], 'Date': [], 'link': []} \n",
    "    \n",
    "    #A loop to extract the data of each product and save it in the dictionary\n",
    "    for link in link_list:\n",
    "        #With this other loop we try to avoid being refused by the server\n",
    "        product_page = ''\n",
    "        while product_page == '':\n",
    "            try:\n",
    "                product_page = requests.get(link, headers=headers)\n",
    "                break\n",
    "            #In case of being refused we put the scraper to sleep a certain time, you can change it if it not works in your case\n",
    "            except:\n",
    "                print(\"Connection refused by the server...\")\n",
    "                print(\"Let me sleep for 120 seconds\")\n",
    "                print(\"Zzzzzz...\")\n",
    "                time.sleep(120)\n",
    "                print(\"Was a nice sleep, now let me continue...\")\n",
    "                continue\n",
    "\n",
    "        #We parse and extract the data of each product webpage\n",
    "        product_soup = BeautifulSoup(product_page.content, \"html.parser\")\n",
    "        \n",
    "        product_dic['Title'].append(get_title(product_soup))\n",
    "        product_dic['Price'].append(get_price(product_soup))\n",
    "        product_dic['Rating'].append(get_rating(product_soup))\n",
    "        product_dic['Reviews'].append(get_reviews(product_soup))\n",
    "        product_dic['Availability'].append(get_availability(product_soup))\n",
    "        #We add the date and link of each product too\n",
    "        product_dic['Date'].append(datetime.date.today())\n",
    "        product_dic['link'].append(link)\n",
    "        #We use time and sleep library to avoid refusing\n",
    "        time.sleep(1)\n",
    "    \n",
    "    #Finaly we convert the dictionary into a dataframe using Pandas, and later into a csv and excel files to save it\n",
    "    amazon_df = pd.DataFrame.from_dict(product_dic)\n",
    "    amazon_df.to_csv(f'{searchterm}.csv', index = False, header = True)\n",
    "    amazon_df.to_excel(f'{searchterm}.xlsx', index = False, header = True)\n",
    "\n",
    "    return amazon_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36da6806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product name: Airpods\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Availability</th>\n",
       "      <th>Date</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Title, Price, Rating, Reviews, Availability, Date, link]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#With this we execute the code and visualize the dataframe\n",
    "\n",
    "product_name = input('Enter the product name: ')\n",
    "\n",
    "amazon_df = amazon_scrapper(product_name)\n",
    "\n",
    "amazon_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d57f1ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
